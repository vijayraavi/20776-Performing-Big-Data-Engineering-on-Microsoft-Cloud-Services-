# Module 7: Implementing Azure SQL Data Warehouse

# Lab: Implementing SQL Data Warehouse

### Scenario
You work for Adatum as a data engineer, and you have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data—captured from speed cameras and automatic number plate recognition (ANPR) devices—and then crosscheck the outputs against large volumes of reference data holding vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that read vehicle registration plates.

In this phase of the project, you will consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse as a single data location for static, or rarely updated, information including stolen vehicle data, vehicle owner data, and speed camera location data. You will also configure the traffic surveillance system to use the same SQL Data Warehouse to hold dynamic data streamed live from the speed cameras.

### Objectives
After completing this lab, you will be able to:
- Create and configure a new SQL Data Warehouse.
- Design and configure SQL Data Warehouse tables.
- Import static data into SQL Data Warehouse.
- Stream dynamic data to SQL Data Warehouse.

### Lab Setup
Estimated time: 90 minutes
Virtual machine: **20776A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

>**Note:** This lab uses the following resources from Lab 5 and earlier:
- **Resource group**: CamerasRG        
- **Data Lake Store**: adls&lt;_your name_&gt;&lt;_date_&gt;
- **Azure Stream Analytics job**: CaptureTrafficData
- **Event Hub**: camerafeeds&lt;_your name_&gt;&lt;_date_&gt;

## Exercise 1: Create and configure a new SQL Data Warehouse

### Scenario
You are going to consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse as a single data location for both static and dynamic data. In this exercise, you will create a new data warehouse for holding traffic data—this warehouse will run on a new database server. You will then use SQL Server Management Studio to explore the data warehouse, and use scaling to set to performance level.

The main tasks for this exercise are as follows:
1. Install AzCopy and AdlCopy
2. Create a new database server
3. Create a new SQL Data Warehouse
4. Explore the SQL Data Warehouse using SQL Server Management Studio
5. Use scaling with SQL Data Warehouse

>**Note**: If you have completed Lab 4, you do not need to complete Exercise 1, Task 1.

#### Task 1: Install AzCopy and AdlCopy
1.  Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, **20776A-LON-SQL**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2.  Install **AzCopy** from **https://docs.microsoft.com/en-us/azure/storage/storage-use-azcopy**.
3.  Add: **C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy** to the system path.
4.  Install **AdlCopy** from **https://www.microsoft.com/en-us/download/details.aspx?id=50358**.
5.  Add **%HOMEPATH%\\Documents\\AdlCopy** to the system path.

#### Task 2: Create a new database server
1.  Using the Azure portal, create a new SQL server (logical server), using the following details:
    - **Server name**: trafficserver&lt;_your name_&gt;&lt;_date_&gt;
    - **Server admin login**: student
    - **Password**: Pa55w.rd
    - **Resource group (use existing)**: CamerasRG
    - **Location**: select the same location as you have used for Data Lake Stores in previous labs
2.  Ensure that the **Allow azure services to access server** check box is selected.
3.  Configure the server firewall with your client IP address, and ensure that **Allow access to Azure services** in set to **ON**.
4.  Wait until the server has deployed before continuing with the lab.

#### Task 3: Create a new SQL Data Warehouse
1.  Using the Azure portal, create a new SQL Data Warehouse, with the following details:
    - **Database name**: trafficwarehouse
    - **Resource group (Use existing)**: CamerasRG
    - **Select source**: Blank database
    - **Server**: trafficserver&lt;_your name_&gt;&lt;_date_&gt;
    - **Performance level**: 100 DWU
    - Leave all other settings at their defaults
2.  Wait until the data warehouse has deployed before continuing with the lab.

#### Task 4: Explore the SQL Data Warehouse using SQL Server Management Studio
1.  Use Microsoft SQL Server Management Studio to connect to your database server, using the following connection details:
    - **Server name**: trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net
    - **Authentication**: SQL Server Authentication
    - **Login**: student
    - **Password**: Pa55w.rd
2.  In Object Explorer, verify that the **trafficwarehouse** data warehouse is listed.
3.  Create a **New Query**, and run the following SELECT statement (you copy the SQL statements in this exercise from **E:\\Labfiles\\Lab07\\Exercise1\\Exercise1.sql**):

    ```
    SELECT *
    FROM sys.dm_pdw_nodes
    GO
    ```

4.  Verify that this select statement lists a single **CONTROL** node and a single **COMPUTE** node.
5.  Make a note of the value in the **pdw\_node\_id** column of the **COMPUTE** node.
6.  Run the following SELECT statement:

    ```
    SELECT *
    FROM sys.pdw_distributions
    GO
    ```

7.  Verify that you see 60 distributions (databases) listed; these distributions should all belong to the COMPUTE node (use the value in the pdw\_node\_id column to verify this).

#### Task 5: Use scaling with SQL Data Warehouse
1.  Use the Azure portal to set the **Performance level** of **trafficwarehouse** to **400**.
2.  Wait until the data warehouse has resumed after the scaling operation before continuing with the lab; this might take several minutes.
3.  When the data warehouse has resumed, return to Microsoft SQL Server Management Studio, reconnect to the data warehouse and repeat the following SELECT statement:

    ```
    SELECT *
    FROM sys.dm_pdw_nodes
    GO
    ```

4.  Verify that, this time, you see four COMPUTE nodes, because you have scaled performance by a factor of four.
5.  Execute the following SELECT statement:

    ```
    SELECT *
    FROM sys.pdw_distributions
    GO
    ```

6.  Verify that you still see 60 distributions but they are now spread across the compute nodes (15 distributions per node).
7.  Use the Azure portal to set the **Performance level** of **trafficwarehouse** back to **100**.
8.  Wait until the data warehouse has resumed after the scaling operation before continuing with the lab; this might take several minutes. It’s important to use an appropriate performance level, as SQL Data Warehouse is an expensive resource, and should only be scaled as needed.

>**Result**: At the end of this exercise, you will have created a new database server, created a new data warehouse, explored the data warehouse using SQL Server Management Studio, and used scaling with data warehouse.

## Exercise 2: Design and configure SQL Data Warehouse tables

### Scenario
You’re going to consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse as a single data location for static and dynamic data. In this exercise, you will design and create the tables and indexes that are required to support the traffic monitoring system.

The main tasks for this exercise are as follows:
1. Design tables and indexes for a SQL Data Warehouse application
2. Use SQL Server Management Server to create data warehouse tables and indexes

#### Task 1: Design tables and indexes for a SQL Data Warehouse application
The traffic monitoring system uses the following information:    
**Traffic camera locations**. There are 500 traffic cameras, recording traffic speeds and capturing vehicle registrations. The location of each camera rarely changes.    
**Vehicle speeds**. This data is continually streamed from the traffic cameras. It comprises the identity of the camera, speed limit (this can vary with time), vehicle speed, and vehicle registration. This data is used to perform analysis of traffic patterns, identify hotspots, and so on. It’s also used to send fixed penalty fines and summonses to owners of vehicles caught speeding, and to notify traffic patrols if a suspect (stolen or false) vehicle registration is captured.    
**Vehicle/owner data**. This dataset contains the current vehicle ownership records for every registered vehicle. This data includes the registration number, together with the name (title, forename, surname), and address (lines 1-4) for every vehicle. Currently, there are approximately 7.7 million vehicles registered, but new vehicles are registered often and older vehicles unregistered as they are written off.    
**Stolen vehicles**. This dataset contains the registration number, date reported stolen, and date recovered (or null if the vehicle is still missing) for every vehicle reported stolen. This data is historical—it’s appended to (and updated as vehicles are recovered), but data is never deleted. There are currently 1.2 million stolen vehicle records (approximately 2 percent of vehicles [150,000] are reported stolen each year, and there are currently eight years of data on record—2010-2017 inclusive). The data is expected to grow at a similar rate in the future.    

***Question 1:***
What distribution policy would be most appropriate for each type of data?    
Answer:    
**Vehicle speeds**. Hashed by camera ID. This is high volume data that is likely to grow quickly. Hashing by camera ID will help to spread the load across the data warehouse but ensure that it is found quickly by analytics that examine traffic patterns.    
**Traffic camera locations**. Replicated (small amounts of data that might need to be joined frequently with vehicle speed data).   
**Vehicle/owner data**. Round robin. This data has a relatively slow-moving footprint. It is often retrieved by vehicle registration (making an argument for hashing by registration number), but queries that determine ownership by address are also frequently performed.    
**Stolen vehicles**: Hashed by registration number. Most of the queries that reference this data retrieve it by registration number.    

***Question 2:***
How might your partition the data (if at all)?    
Answer:    
**Vehicle speeds**. Partition by month. Most queries to this data are likely to be about recent incidents, so the current month's data (and possibly the previous month) are the most likely to be accessed. The data also grows by time, so it’s easy for the data warehouse to determine in which partition new data should go (the current month). Also, older data is eventually archived by month more easily.    
**Traffic camera locations**. No partitions.    
**Vehicle/owner data**. No partitions.    
**Stolen vehicles**. Partition by year. Most records for unrecovered vehicles will be for the current/previous year. Older data is more likely to indicate a recovered date.    

#### Task 2: Use SQL Server Management Server to create data warehouse tables and indexes
1.  Using Microsoft SQL Server Management Studio, write and execute CREATE TABLE statements for each of the required tables. The tables should have the following names and columns (name and type):

    ```
    Table: VehicleSpeed
    ----------------------------
    CameraID: VARCHAR(10)
    SpeedLimit: INT
    Speed: INT
    VehicleRegistration: VARCHAR(7)
    WhenDate: DATETIME
    WhenMonth: INT (required for partitioning)

    Table: CameraLocation
    --------------------------------
    CameraID: VARCHAR(10)
    GPSLocationX: FLOAT
    GPSLocationY: FLOAT

    Table: VehicleOwner
    ----------------------------
    VehicleRegistration: VARCHAR(7)
    Title: VARCHAR(30)
    Forename: VARCHAR(30)
    Surname: VARCHAR(30)
    AddressLine1: VARCHAR(50)
    AddressLine2: VARCHAR(50)
    AddressLine3: VARCHAR(50)
    AddressLine4: VARCHAR(50)

    Table:StolenVehicle
    ---------------------------
    VehicleRegistration: VARCHAR(7)
    DateStolen: DATETIME
    DateRecovered: DATETIME
    YearStolen: INT (required for partitioning)
    ```

Note the following:
    - All columns are mandatory, except for the DateRecovered column in the StolenVehicle table, which must allow nulls.
    - The VehicleSpeed and CameraLocation tables must be created as HEAPs.
    - You should use a CLUSTERED COLUMNSTORE index for the other two tables.
    You copy example statements from **E:\\Labfiles\\Lab07\\Exercise2\\Exercise2.sql**.

2.  Use Object Explorer to verify that the **dbo.VehicleSpeed**, **dbo.CameraLocation**, **dbo.VehicleOwner**, and **dbo.StolenVehicle** tables have been created.
3.  Close the SQL Editor, without saving any changes.


>**Result**: At the end of this exercise, you will have designed tables and indexes for a data warehouse application, and used SQL Server Management Server to create the required data warehouse tables and indexes.

## Exercise 3: Import static data into SQL Data Warehouse

### Scenario
In this exercise, you will consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse as a single data location for static, or rarely updated, information including stolen vehicle data, vehicle owner data, and speed camera location data. You will also import static data into the warehouse. There are three data sources for this exercise:
- Stolen vehicle data in Data Lake Store.
- Vehicle owner data in an on-premises SQL Server database.
- Speed camera location data in a local CSV file.

You will first upload the stolen vehicle data to Data Lake Store (using AzCopy and Adlcopy), as a temporary staging location. You will then create a local SQL Server database for holding vehicle owner data, again as a staging location. You will then upload speed camera location data directly into the data warehouse from a local CSV file using AzCopy, PolyBase, and CTAS (dropping the existing table first, and using CTAS with the same options that were used to create the table in the first place). You will then import the staged stolen vehicle data from Data Lake Store into SQL Data Warehouse—leaving the existing table in place—and then use INSERT INTO to append data to the table.

Finally, you will import the staged vehicle/owner data from the on-premises SQL Server database by using an ADO.NET source and destination with SQL Server Integration Services—again leaving the existing table in place.

The main tasks for this exercise are as follows:
1. Stage data in Data Lake Store prior to SQL Data Warehouse import
2. Stage data in an on-premises SQL Server database prior to SQL Data Warehouse import
3. Import data from a local CSV file into SQL Data Warehouse
4. Import data from Data Lake Store into SQL Data Warehouse
5. Import data from an on-premises SQL Server database into SQL Data Warehouse

#### Task 1: Stage data in Data Lake Store prior to SQL Data Warehouse import
**Note**: This task is only necessary if you have not performed exercise 3 of module 6 which uses the same data. If you have already staged the stolen vehicle data in Data Lake Store, then skip to the next task to stage data in an on-premises SQL Server database.
1.  Use the Azure portal to create a new Blob storage account, using the following details:
    - **Name**: vehicledata&lt;_your name_&gt;&lt;_date_&gt;
    - **Resource group (use existing)**: CamerasRG
    - **Location**: select the same location as you used for the data warehouse in Exercise 1
    - Leave all other details at their defaults
2.  Wait until the storage account has been successfully created before continuing with the exercise.
3.  Add a new blob container named **stolen**.
4.  Make a note of the storage access key for the storage account.
5.  View the **E:\\Labfiles\\Lab07\\Exercise3\\StolenVehicles** folder, and verify that it contains eight years of stolen vehicle data, organized in subfolders by year/month/day; there are 2,914 separate CSV files.
6.  Use the AzCopy command to upload the files and folders under the **E:\\Labfiles\\Lab07\\Exercise3\\StolenVehicles** folder to the blob container. Use the **/S** parameter to indicate that AzCopy should recursively traverse subfolders; you copy the command from **E:\\Labfiles\\Lab07\\Exercise3\\AzCopyCmd1.txt** (replacing **&lt;storage account name&gt;** with **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, and replacing **&lt;storage key&gt;** with the key you noted previously:

    ```
    azcopy /Source:"E:\Labfiles\Lab07\Exercise3\StolenVehicles" /Dest:https://<storage account name>.blob.core.windows.net/stolen /DestKey:<storage key> /S
    ```

7.  The copy process might take several minutes to complete; wait until all files have been copied before continuing with the exercise.
8.  Use Cloud Explorer in Visual Studio to examine the stolen blob container.
9.  In Cloud Explorer, if you do not have an Azure Pass folder, you will need to add the Microsoft account that is associated with your Azure Learning Pass subscription.
10.  Verify that the files and subfolders have been uploaded.
11.  Use the Azure portal to create a new folder called **Stolen** in your Data Lake Store (adls&lt;your name&gt;&lt;date&gt;).
12.  Use the AdlCopy command to transfer the files from Blob storage to the Stolen folder in your Data Lake Store; you copy the following command from **E:\\Labfiles\\Lab07\\Exercise3\\AdlCopyCmd.txt** (replacing **&lt;storage account name&gt;** with **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, replacing **&lt;Data Lake Store name&gt;** with **adls&lt;_your name_&gt;&lt;_date_&gt;**, and replacing **&lt;storage key&gt;** with the blob store key you noted previously):

    ```
    adlcopy /source https://<storage account name>.blob.core.windows.net/stolen/ /dest adl://<Data Lake Store name>.azuredatalakestore.net/Stolen/ /sourcekey <storage key>
    ```

13.  If you get a **Visual Studio** dialog box, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
14.  Note: Depending on the region and the location of the Blob storage account (ideally they should be in the same region, but might not be), AdlCopy will take from two to 20 minutes to copy the data. Ignore the stats that indicate the percentage of files copied—it sits at 0.00% until complete then jumps to 100%, and may copy the data in three phases (files 1 to 1000, then files 1001 to 2000, and then the remainder).
15.  Use Data Explorer in the Azure portal to examine the Data Lake Store and verify that all the files and folders have been copied across.

#### Task 2: Stage data in an on-premises SQL Server database prior to SQL Data Warehouse import
1.  Using Microsoft SQL Server Management Studio, connect to the **LON-SQL** server using Windows Authentication.
2.  Use Object Explorer to create a new database named **VehicleInfo** using the default options.
3.  In a new query window, run the following commands (you copy these from **E:\\Labfiles\\Lab07\\Exercise3\\SqlCmd1.txt)**:

    ```
    USE VehicleInfo
    GO
    -- Create VehicleOwner table
    CREATE TABLE VehicleOwner
    (
    VehicleRegistration VARCHAR(7) NOT NULL,
    Title VARCHAR(30) NOT NULL,
    Forename VARCHAR(30) NOT NULL,
    Surname VARCHAR(30) NOT NULL,
    AddressLine1 VARCHAR(50) NOT NULL,
    AddressLine2 VARCHAR(50) NOT NULL,
    AddressLine3 VARCHAR(50) NOT NULL,
    AddressLine4 VARCHAR(50) NOT NULL
    )
    GO
    ```

4.  At a command prompt, move to the **E:\\Labfiles\\Lab07\\Exercise3** folder, and run the following command (you copy this from **E:\\Labfiles\\Lab07\\Exercise3\\BcpCmd1.txt**):

    ```
    bcp VehicleInfo.dbo.VehicleOwner in ownerdata.csv /T /SLON-SQL /c /t,
    ```

Ensure that you include the comma at the end of the line.

5.  The bcp command should upload more than 7.7 million rows to the database (it’s fairly quick, and should take no more than five minutes).
6.  Return to Microsoft SQL Server Management Studio and run the following query (you copy this from **E:\\Labfiles\\Lab07\\Exercise3\\SqlCmd2.txt**):

    ```
    SELECT TOP(1000) *
    FROM VehicleOwner
    GO
    ```

7.  This command should display the first 1,000 rows of data; the names and addresses contain random strings, but the vehicle registrations tie in with those of the stolen vehicle data (and the data generated by the speed cameras that you will use in Exercise 4).
8.  Close the query window, without saving any changes, and disconnect from LON-SQL.

#### Task 3: Import data from a local CSV file into SQL Data Warehouse
1.  Use the Azure portal to add a new blob container named **locationdata** to the **vehicledata&lt;_your name_&gt;&lt;_date_&gt;** Blob storage account.
2.  Make a note of the storage access key for the storage account.
3.  Use the AzCopy command to upload the **CameraData.csv** file in the **E:\\Labfiles\\Lab07\\Exercise3** folder to the **locationdata** container; you copy the following command from **E:\\Labfiles\\Lab07\\Exercise3\\AzCopyCmd2.txt** (replacing **&lt;storage account name&gt;** with **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, and replacing **&lt;storage key&gt;** with the key you noted in Step 2):

    ```
    azcopy /Source:"E:\Labfiles\Lab07\Exercise3\" /Pattern:CameraData.csv /Dest:https://<storage account name>.blob.core.windows.net/locationdata /DestKey:<storage key>
    ```

4.  In Microsoft SQL Server Management Studio, re-establish the database connection with the correct credentials (SQL Server authentication and username/password student/Pa55w.rd).
5.  In Microsoft SQL Server Management Studio, open **E:\\Labfiles\\Lab07\\Exercise3\\SpeedCameraLocationScript.sql**.
6.  Ensure that **trafficserver&lt;_your name_&gt;&lt;_date_&gt;** is selected, then execute the following command in the script to create an encryption key for encrypting Blob storage credentials (you can use any password):

    ```
    CREATE MASTER KEY ENCRYPTION BY PASSWORD='<password>'
    GO
    ```

7.  Execute the following command in the script to create a database scoped credential for accessing the **vehicledata** storage account; the identity is the name of the storage account (that is, **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**), and the secret is the storage account key:

    ```
    CREATE DATABASE SCOPED CREDENTIAL CredentialsToBlobStorage
    WITH IDENTITY = '<storage account name>',
    SECRET = '<storage key>';
    GO
    ```

8.  Execute the following command in the script to create an external data source named **LocationDataSource** that connects to the **locationdata** container in the **vehicledata** Blob storage account (that is, vehicledata&lt;your name&gt;&lt;date&gt;) using this credential:

    ```
    CREATE EXTERNAL DATA SOURCE LocationDataSource
    WITH (
    TYPE = HADOOP,
    LOCATION = 'wasbs://locationdata@<storage account name>.blob.core.windows.net',
    CREDENTIAL = CredentialsToBlobStorage
    )
    GO
    ```

9.    Execute the following command in the script to define the file format of the data to be read from this data source; it is CSV data:

    ```
    CREATE EXTERNAL FILE FORMAT CommaSeparatedFileFormat
    WITH (
    FORMAT_TYPE = DelimitedText,
    FORMAT_OPTIONS (FIELD_TERMINATOR =',')
    )
    GO
    ```

10.  Execute the following command in the script to create an external table named **ExternalLocationData** that uses the data source and file format to read the location data from Blob storage. Remember that the file in Blob storage is named **CameraData.csv**. The table should contain the following three fields:
 - CameraID: VARCHAR(10) NOT NULL
 - GPSLocationX: FLOAT NOT NULL
 - GPSLocationY: FLOAT NOT NULL

    ```
    CREATE EXTERNAL TABLE ExternalLocationData (
    CameraID VARCHAR(10) NOT NULL,
    GPSLocationX FLOAT NOT NULL,
    GPSLocationY FLOAT NOT NULL)
    WITH (
    LOCATION='CameraData.csv',
    DATA_SOURCE = LocationDataSource,
    FILE_FORMAT = CommaSeparatedFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
    )
    GO
    ```

11.  Execute the following command in the script to verify that the external table has been created correctly, and use it to read the camera locations; the query should return 500 rows:

    ```
    SELECT *
    FROM ExternalLocationData
    GO
    ```

12.  Execute the following command in the script to drop the existing **CameraLocation** table from the data warehouse; you will rebuild it using CTAS in the next step:

    ```
    DROP TABLE CameraLocation
    GO
    ```

13.  Execute the following command in the script to use CTAS to rebuild and populate the **CameraLocation** table; remember that this table should be implemented as a heap using the REPLICATE distribution policy:

    ```
    CREATE TABLE CameraLocation
    WITH
    (
    HEAP,
    DISTRIBUTION = REPLICATE
    )
    AS SELECT CameraID, GPSLocationX, GPSLocationY
    FROM ExternalLocationData
    GO
    ```

14.  Execute the following command in the script to verify that the **CameraLocation** table has been recreated and populated; the following query should return the same 500 rows found in the external table:

    ```
    SELECT *
    FROM CameraLocation
    GO
    ```

15.  Close the query window, without saving any changes.

#### Task 4: Import data from Data Lake Store into SQL Data Warehouse
1.  Use the Azure portal to create a new Application Registration in Azure Active Directory, with the following details:
    - **Name**: ADLSToPolyBase
    - **Application type**: Web app/ API
    - **Sign-on URL**: https://ADLSToPolyBase/Dummy
> Note that the actual URL entered on this blade is immaterial because you do not actually build or deploy an app at this location; it’s merely acting as an identifier in this example.

2.  Open the **ADLSToPolyBase** application and make a note of the **Application ID** (referred to as **&lt;application ID&gt;** later).
3.  Create a new application key named **Key1** that expires in one year. Save the key and make a note of the value (referred to as **&lt;key&gt;** later); you might want to save the Application ID and Key values to Notepad for reference later).
4.  In the Properties of your **Azure Active Directory** blade, locate your **Directory ID** (referred to as **&lt;directory ID&gt;** later), and make a note of this ID, or save it to Notepad.
5.  Use Data Explorer for your Data Lake Store (adls&lt;your name&gt;&lt;date&gt;), to add the following access permissions to the root folder for ADLSToPolyBase:
    - **Read**, **Write**, and **Execute** permissions, for **This folder and all children**.
6.  **IMPORTANT**. Wait while permissions are assigned to **ADLSPolyBase**; notice the assignments to each file and folder is displayed in the **Assigning permissions to ...** box, in the **Access** blade. Do not continue with the exercise until you see the notification that the permissions assignment has successfully completed (this might take several minutes).
7.  In Microsoft SQL Server Management Studio, open **E:\\Labfiles\\Lab07\\Exercise3\\StolenVehicleDataScript.sql**.
8.  Ensure that **trafficserver*&lt;your name&gt;&lt;date&gt; ***is selected, and then execute the following command in the script to create another database scoped credential, this time for accessing the ADLA account, replacing **&lt;application ID&gt;**, **&lt;directory ID&gt;**, and **&lt;key&gt;** with the values you noted in Step 3:

    ```
    CREATE DATABASE SCOPED CREDENTIAL ADLCredential
    WITH
    IDENTITY = '<application ID>@https://login.windows.net/<directory ID>/oauth2/token',
    SECRET = '<key>'
    GO
    ```

9.  Execute the following command in the script to create an external data source named **StolenVehicleDataSource** that connects to the Data Lake storage account using this credential, replacing **&lt;Data Lake Store name&gt;** with **adls&lt;_your name_&gt;&lt;_date_&gt;**:

    ```
    CREATE EXTERNAL DATA SOURCE StolenVehicleDataSource
    WITH (
    TYPE = HADOOP,
    LOCATION = 'adl://<Data Lake Store name>.azuredatalakestore.net',
    CREDENTIAL = ADLCredential
    )
    GO
    ```

10. Execute the following command in the script to define the file format of the data to be read from this data source; it is CSV data but some of the strings are delimited with quotes that should be not included in the data:

    ```
    CREATE EXTERNAL FILE FORMAT DelimitedCsvTextFileFormat
    WITH (
    FORMAT_TYPE = DelimitedText,
    FORMAT_OPTIONS (
    FIELD_TERMINATOR = ',',
    STRING_DELIMITER = '"'
    ));
    GO
    ```

11. Execute the following command in the script to create an external table named **ExternalStolenVehicleData** that uses the data source. The table should contain the following three fields:
 - VehicleRegistration: VARCHAR(7) NOT NULL
 - DateStolen: VARCHAR(25) NOT NULL
 - DateRecovered: VARCHAR(25) NULL

    ```
    CREATE EXTERNAL TABLE ExternalStolenVehicleData (
    VehicleRegistration VARCHAR(7) NOT NULL,
    DateStolen DATETIME NOT NULL,
    DateRecovered DATETIME NULL )
    WITH (
    LOCATION='/Stolen/',
    DATA_SOURCE = StolenVehicleDataSource,
    FILE_FORMAT = DelimitedCsvTextFileFormat,
    REJECT_TYPE = PERCENTAGE,
    REJECT_VALUE = 5,
    REJECT_SAMPLE_VALUE = 1000
    );
    GO
    ```

12. It might take a couple of minutes to create this table. Note that:
    - DateRecovered must allow NULL values.
    - The date fields are defined as VARCHAR rather than DATETIME in the external table. When you transfer the data into the data warehouse you will convert this data, and also extract the year from the DateStolen field to use as the partition key.
    - Not all of the rows in each file in ADLS contain valid data (there are some header rows, and there could also be some other forms of corruption, given the volume of the data). Specify that you will allow up to 5 percent of the rows retrieved by a query to be discarded if they do not appear to contain valid data. Sample over every 1,000 rows.
13. Execute the following command in the script to verify that the external table has been created correctly, by reading the first 1,000 rows of stolen vehicle data:

    ```
    SELECT TOP(1000) *
    FROM ExternalStolenVehicleData
    GO
    ```

> This query is likely to be slow (it might take three or four minutes).
14. Execute the following command in the script to use an INSERT INTO statement to populate the **StolenVehicle** table; this statement should convert the VARCHAR date columns in the external table into DATETIME values, and also extract the year from the DateStolen column in the external table, and store it in the YearStolen column in the StolenVehicle table:

    ```
    INSERT INTO StolenVehicle(VehicleRegistration, DateStolen, DateRecovered, YearStolen)
    SELECT VehicleRegistration, CONVERT(DATETIME, DateStolen), CONVERT(DATETIME, DateRecovered), DATEPART(yyyy, CONVERT(DATETIME, DateStolen))
    FROM ExternalStolenVehicleData
    GO
    ```

15. This operation will take some time to perform (three or four minutes); additionally, you should see that 2,914 rows are rejected (these are the rows containing headers rather than data)—this is normal.
16. Execute the following command in the script to verify that the **StolenVehicle** table has been populated; the query should return 1,000 rows, but will be much faster than querying the data from ADLS via PolyBase:

    ```
    SELECT TOP(1000) *
    FROM StolenVehicle
    GO
    ```

17. Close the query window, without saving any changes.

#### Task 5: Import data from an on-premises SQL Server database into SQL Data Warehouse
1.  Close any existing instances of Visual Studio, and then install **SQL Server Data Tools (SSDT) for Visual Studio 2017** from **https://go.microsoft.com/fwlink/?linkid=853836**. In the installer, select the existing installation of Visual Studio, and select all the installation options. The install might take 10-15 minutes, and when it’s completed, restart 20776A-LON-DEV.
2.  Install **SSIS Feature Pack for Azure (SQL Server 2017)** from **https://www.microsoft.com/en-us/download/details.aspx?id=54798**, selecting the 64-bit installer.
3.  In Visual Studio 2017, create a new Integration Service Project called **ImportVehicleOwnerData**.
4.  When the project has been created, drag a **Data Flow Task** from the SSIS Toolbox onto the Design window.
5.  Click the **Data Flow** tab at the top of the Design pane.
6.  In the SSIS Toolbox, expand **Other Sources**, and then drag the **ADO.NET Source** onto the Data Flow window.
7.  Open the **ADO.NET Source** component to display the **ADO.NET Source Editor**.
8.  Click **New** to display the **Configure ADO.NET Connection Manager** dialog box.
9.  Click **New** to display the **Connection Manager** dialog box, and enter the following details:
    - **Server name**: LON-SQL
    - **Connect to a database**: VehicleInfo
10.  In the **Connection Manager** dialog box, click **OK**.
11.  In the ADO.NET Source Editor, in the **Name of the table or the view drop-down** **list** box, click **dbo.VehicleOwner**, and then click **OK**.
12.  In the SSIS Toolbox, expand **Other Destinations**, and then drag the **ADO.NET Destination** onto the Data Flow window.
13.  Click the **ADO.NET Source** component, and then click and drag the blue connector to the **ADO.NET Destination**.
14.  Open the **SQL Server Destination** component to display the **ADO.NET Destination Editor**.
15.  By the **Connection Manager** drop-down list box, click **New** to display the **Configure ADO.NET Connection Manager** dialog box.
16.  Click **New** to display the **Connection Manager** dialog box, and enter the following details:
    - **Server Name**: trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net
    - SQL Server Authentication
    - **Username**: student
    - **Password**: Pa55w.rd
    - Select **Save my password**
    - **Database Name**: trafficwarehouse
17.  In the **Configure ADO.NET Connection Manager** dialog box, click **OK**.
18.  In the **ADO.NET Destination Editor** window, in the **Use a table or view** drop-down list box, click **[dbo].[VehicleOwner]**, and then click **Mappings**.
19.  Verify that all of the columns in the source table are mapped to the corresponding columns in the destination table, and then click **OK**.
20.  Save the project.
21.  On the **Debug** menu, click **Start Debugging** to run the package and perform the upload.
22.  Close Visual Studio.

>**Result**: At the end of this exercise, you will have:
- Staged data in Data Lake Store prior to SQL Data Warehouse import.
- Staged data in an on-premises SQL Server database prior to SQL Data Warehouse import.
- Imported data from a local CSV file directly into SQL Data Warehouse.
- Imported data from Data Lake Store into SQL Data Warehouse.
- Imported data from an on-premises SQL Server database into SQL Data Warehouse.

## Exercise 4: Stream dynamic data to SQL Data Warehouse

### Scenario
In this exercise, you will consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse as a single data location for dynamic data streamed live from the speed cameras. You will stream dynamic data from speed cameras into SQL Data Warehouse from a Stream Analytics job, leaving the existing table in place. You will then configure a Visual Studio app to use this Stream Analytics job, and then view the Stream Analytics job data in SQL Data Warehouse.

The main tasks for this exercise are as follows:
1. Configure an Azure Stream Analytics job to output to SQL Data Warehouse
2. Configure a Visual Studio app to use the Stream Analytics job
3. View Stream Analytics job data in SQL Data Warehouse
4. Lab cleanup

#### Task 1: Configure an Azure Stream Analytics job to output to SQL Data Warehouse
1.  Using the Azure portal, open the **CaptureTrafficData** Azure Stream Analytics job; remember that this job captures data from the speed cameras and writes it to Data Lake storage.
2.  Add another output to the job, using the following details:
    - **Output alias**: DataWarehouse
    - **Sink**: SQL database
    - **Import option**: Use SQL database from current subscription
    - **Database**: trafficwarehouse
    - **Username**: student
    - **Password**: Pa55w.rd
    - **Table**: VehicleSpeed
    - Leave all other settings at their defaults
3.  Wait until the output has been successfully created before continuing with the lab.
4.  Edit the query for the job, and add the following SELECT statement:

    ```
    SELECT
    CameraID, SpeedLimit, Speed, VehicleRegistration, Time AS WhenDate, DATEPART(month, Time) AS WhenMonth
    INTO
    DataWarehouse
    FROM
    CameraDataFeed4
    ```

You copy the preceding commands from **E:\\Labfiles\\Lab07\\Exercise4\\ASAquery.txt**.
5.  Note that the names of the columns or aliases in the SELECT statement must match the names of the columns in the table in the SQL Data Warehouse; remember that the VehicleSpeed table uses the WhenMonth column to partition the data.
6.  Start the **TrafficAnalytics** job.
7.  Wait until the job has been successfully started before continuing with the lab.

#### Task 2: Configure a Visual Studio app to use the Stream Analytics job
1.  When the job is running, go to Visual Studio, and open the **SpeedCameraDevice** solution from the **E:\\Labfiles\\Lab07\\Exercise4\\SpeedCameraDevice** folder.
2.  Edit the **app.config** file and add the Shared Access Key for the event hub to the **Microsoft.ServiceBus.ConnectionString** setting, and replace the value in the **EventHubName** setting with your event hub. Leave the remaining parameters unchanged; you copied to these values to **Config\_details.txt** in Lab 2, Exercise 3.
3.  Note that this version of the application captures data from 500 speed cameras.
4.  Set **SpeedCameraDriver** as the startup project, and then build the solution.
5.  Verify that the app compiles successfully then start the app.
6.  Verify that the app opens a console window displaying generated speed camera data that is being sent to the event hub.

#### Task 3: View Stream Analytics job data in SQL Data Warehouse
1.  In Microsoft SQL Server Management Studio, connect to **trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net** as **student**, with the password **Pa55w.rd**.
2.  Use Object Explorer to run a **Select Top 1000 Rows** query against **dbo.VehicleSpeed**.
3.  Verify that the first 1000 rows of data are shown.

#### Task 4: Lab cleanup
1.  Stop the **SpeedCameraDevice** application.
2.  Close Visual Studio and SQL Server Management Studio.
3.  Stop the **CaptureTrafficData** Stream Analytics job.
4.  VERY IMPORTANT: Pause the **trafficwarehouse** data warehouse; if you don't do this, you will quickly run out of Azure credits!
5.  Remember that resources are billed for all the time that the data warehouse is running, even if it is not actively performing a task. If you are not going to use the data warehouse for a while, you should click **Pause** to stop the warehouse and release resources. You will not be billed for the time the data warehouse is paused.

>**Result**: At the end of this exercise, you will have configured a Stream Analytics job to output to SQL Data Warehouse, configured a Visual Studio app to use the Stream Analytics job, and viewed Stream Analytics job data in SQL Data Warehouse.

**Question**: In the table design, why is it recommended that the vehicle speed data is hashed by camera ID?

**Question**: What are two of the most important management options for SQL Data Warehouse that help control your Azure costs?

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
